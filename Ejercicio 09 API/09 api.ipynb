{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f5c3f8b",
   "metadata": {},
   "source": [
    "# Ejercicio 9: Uso de la API de OpenAI\n",
    "\n",
    "En este ejercicio vamos a aprender a utilizar la API de OpenAI\n",
    "\n",
    "## 1. Uso básico\n",
    "\n",
    "El siguiente código sirve para conectarse con la API de OpenAI de forma básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb87e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6af4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the silver glow of the moon, the whimsical unicorn danced through a forest of twinkling stars, leaving trails of laughter for dreamers everywhere.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "API = \"APIKEY\"\n",
    "client = OpenAI(api_key=API)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eef874",
   "metadata": {},
   "source": [
    "## 2. Retrieval\n",
    "### 2.1 Cargo el corpus de 20 News Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8019fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroupsdocs = newsgroups.data[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6450fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51133cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>The following are my thoughts on a meeting tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>David posts a good translation of a post by Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Note: I am cross-posting (actually, emailing) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>\\nThen don't complain (maybe it wasn't you) th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Since the losers that sold me the hard disk fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    raw\n",
       "0     \\n\\nI am sure some bashers of Pens fans are pr...\n",
       "1     My brother is in the market for a high-perform...\n",
       "2     \\n\\n\\n\\n\\tFinally you said what you dream abou...\n",
       "3     \\nThink!\\n\\nIt's the SCSI card doing the DMA t...\n",
       "4     1)    I have an old Jasmine drive which I cann...\n",
       "...                                                 ...\n",
       "4995  The following are my thoughts on a meeting tha...\n",
       "4996  David posts a good translation of a post by Su...\n",
       "4997  Note: I am cross-posting (actually, emailing) ...\n",
       "4998  \\nThen don't complain (maybe it wasn't you) th...\n",
       "4999  Since the losers that sold me the hard disk fo...\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_df = pd.DataFrame(newsgroupsdocs, columns=['raw'])\n",
    "newsgroups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea70d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo SBERT\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(newsgroupsdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11953fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.002078  ,  0.02345043,  0.02480886, ...,  0.0014359 ,\n",
       "         0.01510752,  0.05287581],\n",
       "       [ 0.05006031,  0.02698093, -0.00886484, ..., -0.00887169,\n",
       "        -0.06737083,  0.05656362],\n",
       "       [ 0.01640475,  0.08100051, -0.04953596, ..., -0.04184629,\n",
       "        -0.07800221, -0.03130953],\n",
       "       ...,\n",
       "       [-0.01800568,  0.03764157,  0.0190541 , ..., -0.02706385,\n",
       "         0.06655327, -0.0518438 ],\n",
       "       [-0.02670172, -0.01402128,  0.00013569, ..., -0.02312811,\n",
       "        -0.02680116, -0.03862799],\n",
       "       [ 0.00509074, -0.01956049, -0.05848601, ...,  0.12941848,\n",
       "        -0.06473386, -0.01300098]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0367a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>[0.0020780046470463276, 0.02345043234527111, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>[0.05006030574440956, 0.0269809328019619, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>[0.016404753550887108, 0.08100050687789917, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>[-0.01939147524535656, 0.011494365520775318, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>[-0.03928707540035248, -0.05540286749601364, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>The following are my thoughts on a meeting tha...</td>\n",
       "      <td>[-0.04432957246899605, 0.042666345834732056, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>David posts a good translation of a post by Su...</td>\n",
       "      <td>[0.01835048384964466, 0.1009376272559166, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Note: I am cross-posting (actually, emailing) ...</td>\n",
       "      <td>[-0.01800568215548992, 0.03764156997203827, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>\\nThen don't complain (maybe it wasn't you) th...</td>\n",
       "      <td>[-0.02670172043144703, -0.014021284878253937, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Since the losers that sold me the hard disk fo...</td>\n",
       "      <td>[0.005090739578008652, -0.019560493528842926, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    raw  \\\n",
       "0     \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1     My brother is in the market for a high-perform...   \n",
       "2     \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3     \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4     1)    I have an old Jasmine drive which I cann...   \n",
       "...                                                 ...   \n",
       "4995  The following are my thoughts on a meeting tha...   \n",
       "4996  David posts a good translation of a post by Su...   \n",
       "4997  Note: I am cross-posting (actually, emailing) ...   \n",
       "4998  \\nThen don't complain (maybe it wasn't you) th...   \n",
       "4999  Since the losers that sold me the hard disk fo...   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [0.0020780046470463276, 0.02345043234527111, 0...  \n",
       "1     [0.05006030574440956, 0.0269809328019619, -0.0...  \n",
       "2     [0.016404753550887108, 0.08100050687789917, -0...  \n",
       "3     [-0.01939147524535656, 0.011494365520775318, -...  \n",
       "4     [-0.03928707540035248, -0.05540286749601364, -...  \n",
       "...                                                 ...  \n",
       "4995  [-0.04432957246899605, 0.042666345834732056, -...  \n",
       "4996  [0.01835048384964466, 0.1009376272559166, 0.00...  \n",
       "4997  [-0.01800568215548992, 0.03764156997203827, 0....  \n",
       "4998  [-0.02670172043144703, -0.014021284878253937, ...  \n",
       "4999  [0.005090739578008652, -0.019560493528842926, ...  \n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_df['embeddings'] = embeddings.tolist()\n",
    "newsgroups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a452eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5b9fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.40027751e-02  6.76465631e-02  1.81450676e-02  2.71494035e-02\n",
      "  3.97355147e-02  2.86677741e-02  1.70837287e-02 -5.11704050e-02\n",
      " -7.08375424e-02 -5.84035367e-03 -2.68768277e-02  9.36001390e-02\n",
      " -5.38061373e-03 -6.05956353e-02 -7.20592961e-02  4.52284105e-02\n",
      " -1.30038504e-02 -4.73392345e-02 -2.32526548e-02  4.68500927e-02\n",
      "  1.05721936e-01  4.48408648e-02  1.61421206e-02  6.92704171e-02\n",
      " -3.10235526e-02  6.19343705e-02  5.20551577e-03 -3.54517475e-02\n",
      "  1.06270779e-02 -3.20176929e-02 -9.43932403e-03 -9.78435203e-03\n",
      "  5.84899709e-02  1.47197843e-02 -1.96531881e-02 -1.94037799e-02\n",
      "  1.74149409e-01 -1.98645946e-02  2.77989581e-02 -1.10337818e-02\n",
      "  1.38298701e-02 -1.01271942e-01  1.29697900e-02  8.35139528e-02\n",
      " -7.40730716e-03  1.14514142e-01 -4.31395769e-02  2.95909438e-02\n",
      " -3.40581052e-02  1.38381766e-02 -1.20901493e-02  8.85839462e-02\n",
      " -3.82152312e-02 -1.93621740e-02  7.27589726e-02  7.54610524e-02\n",
      " -1.64738670e-02  1.54060200e-02 -2.94414982e-02 -7.66353235e-02\n",
      "  6.28036121e-03 -2.38865763e-02  2.20041256e-03  3.95378619e-02\n",
      "  1.20463975e-01 -3.11193876e-02 -4.95886104e-03  3.17789643e-04\n",
      " -7.16621503e-02 -5.91738410e-02 -3.53525691e-02  1.76352914e-02\n",
      "  4.70140157e-03 -8.05521198e-03 -6.35739462e-03 -1.18630692e-01\n",
      "  2.65875123e-02  5.88332638e-02  1.31696388e-02 -4.98005264e-02\n",
      "  3.97385806e-02  7.46968528e-03  2.43806653e-02 -2.15185955e-02\n",
      " -1.79519430e-02 -3.10683865e-02 -8.48300848e-03 -6.22171499e-02\n",
      " -4.98611340e-03  2.30788346e-02  1.26482807e-02 -1.25394881e-01\n",
      "  7.33378679e-02 -6.07658643e-03 -1.73190199e-02  2.85507441e-02\n",
      " -3.96713242e-03 -6.11358695e-02 -1.12080388e-02  3.50211412e-02\n",
      "  1.01146542e-01  4.64464054e-02  5.64822555e-03 -3.39607447e-02\n",
      " -5.08458540e-02 -5.49458414e-02 -4.95848395e-02 -6.10757247e-03\n",
      "  8.77952855e-03  2.38018539e-02  2.35664602e-02 -4.59958874e-02\n",
      " -1.65306643e-01 -7.87507463e-03 -6.83233794e-03 -8.73022825e-02\n",
      "  7.10017905e-02 -1.21346135e-02  7.70054711e-03 -4.71754894e-02\n",
      " -2.80767810e-02 -8.79790820e-03 -9.76419598e-02 -3.57528888e-02\n",
      " -5.32729328e-02 -8.01573321e-02 -4.94632125e-02 -6.45495907e-33\n",
      " -9.89053026e-03 -5.31087406e-02 -2.94314399e-02  5.94666377e-02\n",
      " -6.61945120e-02  6.57089613e-03 -4.76221414e-03 -2.87787206e-02\n",
      " -6.88636154e-02  4.95636947e-02  2.88693104e-02  6.66784123e-02\n",
      "  4.28235270e-02  9.28190798e-02 -2.83197034e-02 -2.88340990e-02\n",
      " -1.07650332e-01  9.79247391e-02  8.66113678e-02 -5.21682426e-02\n",
      "  5.77555783e-02  5.80974035e-02  2.02721916e-03 -5.87115288e-02\n",
      "  4.38670292e-02  1.25902146e-02 -7.52158761e-02  3.77746648e-04\n",
      " -3.04468349e-02 -1.10026426e-03 -1.43295880e-02  8.66110921e-02\n",
      "  1.20158652e-02 -2.88943686e-02  5.73755428e-02  4.37455177e-02\n",
      " -1.91115963e-04 -2.83007901e-02 -1.39162084e-02 -7.79926032e-02\n",
      " -1.94522869e-02  2.88932258e-03  4.04087752e-02  5.81380762e-02\n",
      "  1.22104421e-01  5.07144593e-02  4.19749990e-02  5.82063235e-02\n",
      "  3.80660109e-02 -5.76548232e-03 -2.20759795e-03  1.39616877e-02\n",
      " -1.16940197e-02 -5.49868047e-02 -2.42949427e-05 -8.34286027e-03\n",
      " -6.27730861e-02  4.95420732e-02 -2.40138005e-02 -2.62081642e-02\n",
      " -4.02904525e-02  5.06908149e-02 -5.13635166e-02  3.29045616e-02\n",
      " -8.70747492e-02  1.25089018e-02 -8.75166655e-02  3.29238400e-02\n",
      "  1.46101676e-02  2.16761455e-02 -2.33839750e-02  2.21244395e-02\n",
      "  4.30644527e-02  2.39260420e-02 -1.40682021e-02  6.90585896e-02\n",
      "  9.95942801e-02 -2.50984766e-02  1.10253715e-03 -7.31098354e-02\n",
      " -1.34247795e-01 -3.54881100e-02  8.54934081e-02  3.62669230e-02\n",
      "  1.93831616e-03  5.96100017e-02  5.65770715e-02 -2.61839833e-02\n",
      "  4.05610092e-02 -4.05908339e-02 -1.24699455e-02  2.38104556e-02\n",
      " -1.00872815e-02 -1.87816601e-02  2.07631532e-02  1.73083866e-33\n",
      " -6.61344677e-02 -2.61770450e-02 -7.69421011e-02 -8.21409188e-03\n",
      " -3.14759947e-02 -2.44930722e-02 -1.42561020e-02  5.93728013e-02\n",
      " -1.54998936e-02 -5.88783808e-02 -1.04663079e-03 -4.12924401e-02\n",
      "  1.15597226e-01  9.90181640e-02 -5.60927577e-02 -2.75498424e-02\n",
      "  6.46048710e-02 -3.86647619e-02 -1.45429326e-02  1.85461789e-02\n",
      " -8.30103736e-03  2.91500986e-02 -2.13217027e-02 -1.31738782e-02\n",
      "  1.89553779e-02  2.87054642e-03 -2.38846485e-02 -1.23748928e-01\n",
      "  6.76316470e-02 -7.27729127e-02  3.64424959e-02 -3.73961814e-02\n",
      " -6.97739273e-02  6.26837760e-02 -7.16754645e-02 -1.90311894e-02\n",
      "  6.69729784e-02  6.06542407e-03  2.73328722e-02 -5.10890968e-02\n",
      "  3.56831998e-02 -1.94262564e-02 -4.06893417e-02  6.59846980e-03\n",
      " -1.08701959e-01  4.48713079e-02  4.81100567e-02  6.21019043e-02\n",
      " -1.18398264e-01  2.20911519e-04 -7.28533650e-03 -2.45491019e-03\n",
      "  5.38642611e-03 -6.69279089e-03 -7.02738166e-02  9.88697540e-03\n",
      " -6.20936826e-02 -3.60935973e-03  6.08310401e-02  3.50055695e-02\n",
      " -2.17241887e-03  1.03166260e-01 -1.01376228e-01  9.11742300e-02\n",
      "  2.20732223e-02 -4.90955897e-02 -3.50575075e-02  8.86037573e-03\n",
      "  7.10694641e-02  2.41937004e-02  4.80516907e-03 -8.67981911e-02\n",
      " -7.31253326e-02 -6.16153963e-02  8.75844732e-02  1.40627977e-02\n",
      "  1.46576194e-02 -5.81329428e-02 -7.18038678e-02  6.72090799e-04\n",
      "  9.22572836e-02 -1.37538614e-03 -1.21354051e-01  3.21392082e-02\n",
      "  4.16278057e-02 -3.48212523e-03 -4.02923534e-03 -2.60504317e-02\n",
      "  4.60811378e-03  5.30512109e-02 -9.47184786e-02  2.60015186e-02\n",
      " -1.15529738e-01  3.27883177e-02 -5.63707128e-02 -1.66580243e-08\n",
      "  1.73840020e-02  1.75190519e-03 -5.48511669e-02 -5.23687638e-02\n",
      " -2.42676158e-02 -2.02662386e-02  5.43498024e-02 -3.25912461e-02\n",
      " -4.77570035e-02  2.43179817e-02  5.12982868e-02 -3.10845058e-02\n",
      " -4.89284359e-02 -5.39824180e-02 -1.00859357e-02  6.49780482e-02\n",
      " -3.28510697e-03  1.71304662e-02 -1.77233387e-02  1.52061544e-02\n",
      "  2.44268943e-02  1.45258084e-02 -2.88869650e-03 -3.85358073e-02\n",
      "  9.67296027e-03  9.84460711e-02  6.74826503e-02  6.31225854e-02\n",
      "  1.46233812e-02  9.94315185e-03 -1.17941503e-03  7.30133578e-02\n",
      " -7.48073310e-02 -1.86537893e-03 -2.46431567e-02  2.17140634e-02\n",
      " -1.60046294e-02 -2.26055868e-02  5.70021830e-02 -1.13985024e-01\n",
      " -2.68647745e-02  8.61645043e-02  7.06593832e-03  4.57239598e-02\n",
      "  4.68597934e-02 -3.93768772e-02 -5.31262457e-02  3.08215749e-02\n",
      "  1.62207894e-02  3.57825533e-02 -6.20751418e-02 -4.61846367e-02\n",
      "  2.98488457e-02  2.04645656e-02  5.81776574e-02  5.95278218e-02\n",
      "  5.44227697e-02  9.41118225e-03  1.53745757e-02 -2.12296704e-03\n",
      "  5.25686517e-02  8.44901353e-02  6.75498769e-02 -1.47658619e-04]\n"
     ]
    }
   ],
   "source": [
    "# 1. Generar el embedding de la consulta\n",
    "#query = \"What technology did the US benefit from after WW2?\"\n",
    "query = \"What this document explain about United States?\"\n",
    "query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff9973a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calcular la similitud del coseno\n",
    "cosine_scores = cos_sim(query_embedding, embeddings)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3e3b3",
   "metadata": {},
   "source": [
    "Obtengo los 5 documentos más similares a mi query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30196c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.4317, 0.4317, 0.4253, 0.4242, 0.4124]),\n",
       "indices=tensor([2120, 4262, 1685, 1599, 2501]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Obtener los 5 documentos más relevantes (top-k)\n",
    "k = 5\n",
    "top_results = torch.topk(cosine_scores, k=k)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b436136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONTEXTO RECUPERADO ---\n",
      "From: Center for Policy Research <cpr>\n",
      "Subject: Unconventional peace proposal\n",
      "\n",
      "\n",
      "A unconventional proposal for peace in the Middle-East.\n",
      "---------------------------------------------------------- by\n",
      "\t\t\t  Elias Davidsson\n",
      "\n",
      "The following proposal is based on the following assumptions:\n",
      "\n",
      "1.      Fundamental human rights, such as the right to life, to\n",
      "education, to establish a family and have children, to human\n",
      "dignity, the right to free movement, to free expression, etc. are\n",
      "more important to human existence that the rights of states.\n",
      "\n",
      "2.      In the event of a conflict between basic human rights and\n",
      "rights of collectivities, basic human rights should prevail.\n",
      "\n",
      "3.      Between the collectivities defining themselves as\n",
      "Jewish-Israeli and Palestinian-Arab, however labelled, an\n",
      "unresolved conflict exists.\n",
      "\n",
      "4.      This conflict has caused great sufferings for millions of\n",
      "people. It moreover poisons relations between communities, peoples\n",
      "and nations.\n",
      "\n",
      "5.      Each year, the United States expends billions of dollars\n",
      "in economic and military aid to the conflicting parties.\n",
      "\n",
      "6.      Attempts to solve the Israeli-Arab conflict by traditional\n",
      "political means have failed.\n",
      "\n",
      "7.      As long as the conflict is perceived as that between two\n",
      "distinct ethnical/religious communities/peoples which claim the\n",
      "land, there is no just nor peaceful solution possible.\n",
      "\n",
      "8.      Love between human beings can be capitalized for the sake\n",
      "of peace and justice. When people love, they share.\n",
      "\n",
      "Having stated my assumptions, I will now state my proposal.\n",
      "\n",
      "1.      A Fund should be established which would disburse grants\n",
      "for each child born to a couple where one partner is Israeli-Jew\n",
      "and the other Palestinian-Arab.\n",
      "\n",
      "2.      To be entitled for a grant, a couple will have to prove\n",
      "that one of the partners possesses or is entitled to Israeli\n",
      "citizenship under the Law of Return and the other partner,\n",
      "although born in areas under current Isreali control, is not\n",
      "entitled to such citizenship under the Law of Return.\n",
      "\n",
      "3.      For the first child, the grant will amount to $18.000. For\n",
      "the second the third child, $12.000 for each child. For each\n",
      "subsequent child, the grant will amount to $6.000 for each child.\n",
      "\n",
      "\n",
      "4.      The Fund would be financed by a variety of sources which\n",
      "have shown interest in promoting a peaceful solution to the\n",
      "Israeli-Arab conflict, including the U.S. Government, Jewish and\n",
      "Christian organizations in the U.S.  and a great number of\n",
      "governments and international organizations.\n",
      "\n",
      "5.      The emergence of a considerable number of 'mixed'\n",
      "marriages in Israel/Palestine, all of whom would have relatives on\n",
      "'both sides' of the divide, would make the conflict lose its\n",
      "ethnical and unsoluble core and strengthen the emergence of a\n",
      "truly civil society. The existence of a strong 'mixed' stock of\n",
      "people would also help the integration of Israeli society into the\n",
      "Middle-East in a graceful manner.\n",
      "\n",
      "Objections to this proposal will certainly be voiced. I will\n",
      "attempt to identify some of these:\n",
      "\n",
      "1.      The idea of providing financial incentives to selected\n",
      "forms of partnership and marriage, is not conventional. However,\n",
      "it is based on the concept of affirmative action, which is\n",
      "recognized as a legitimate form of public policy to reverse the\n",
      "perverse effects of segregation and discrimination. International\n",
      "law clearly permits affirmative action when it is aimed at\n",
      "reducing racial discrimination and segregation.\n",
      "\n",
      "2.      It may be objected that the Israeli-Palestinian conflict\n",
      "is not primarily a religious or ethnical conflict, but that it is\n",
      "a conflict between a colonialist settler society and an indigenous\n",
      "colonized society that can only regain its freedom by armed\n",
      "struggle. This objection is based on the assumption that the\n",
      "'enemy' is not Zionism as ideology and practice, but\n",
      "Israeli-Jewish society and its members which will have to be\n",
      "defeated. This objection has no merit because it does not fulfill\n",
      "the first two assumptions concerning the primacy of fundamental\n",
      "human rights over collective rights (see above)\n",
      "\n",
      "3.      Fundamentalist Jews would certainly object to the use of\n",
      "financial incentives to encourage 'mixed marriages'. From their\n",
      "point of view, the continued existence of a specific Jewish People\n",
      "overrides any other consideration, be it human love, peace of\n",
      "human rights.  The President of the World Jewish Congress, Edgar\n",
      "Bronfman, reflected this view a few years ago in an interview he\n",
      "gave to Der Spiegel, a German magazine. He called the increasing\n",
      "assimilation of Jews in the world a <calamity>, comparable in its\n",
      "effects only with the Holocaust. This objection has no merit\n",
      "either because it does not fulfill the first two assumptions (see\n",
      "above)\n",
      "\n",
      "4.      It may objected that only a few people in\n",
      "Israel/Palestine, would request such grants and that it would thus\n",
      "not serve its purpose. To this objection one might respond that\n",
      "although it is not possible to determine with certainty the effect\n",
      "of such a proposal, the existence of such a Fund would help mixed\n",
      "couples to resist the pressure of their respective societies and\n",
      "encourage young couples to reject fundamentalist and racist\n",
      "attitudes.\n",
      "\n",
      "5.      It may objected that such a Fund would need great sums to\n",
      "bring about substantial demographic changes. This objection has\n",
      "merits. However, it must be remembered that huge sums, more than\n",
      "$3 billion, are expended each year by the United States government\n",
      "and by U.S. organizations to maintain an elusive peace in the\n",
      "Middle-East through armaments. A mere fraction of these sums would\n",
      "suffice to launch the above proposal and create a more favorable\n",
      "climate towards the existence of 'mixed' marriages in\n",
      "Israel/Palestine, thus encouraging the emergence of a\n",
      "non-segregated society in that worn-torn land.\n",
      "\n",
      "I would be thankful for critical comments to the above proposal as\n",
      "well for any dissemination of this proposal for meaningful\n",
      "discussion and enrichment.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "I've been reading this board passively for a while now and find the\n",
      "subject absolutely fascinating, especially from the point of view of\n",
      "a civil rights nut like myself.  My problem is that I'm new to the field\n",
      "and paragraphs like the above keep popping up.  I'm sure what Mr. \n",
      "Bellovin is writing about is both fascinating and important, but I have \n",
      "NO IDEA what it means.  :-)   \n",
      "  \n",
      "  Anyway I'm keen to learn and will read anything I can get my hands on\n",
      "that explains this stuff in lay terms (I have a decent CS background, but\n",
      "not a huge amount of hyper-advanced math).  Can anyone point me to a FAQ\n",
      "or a decent source of information about the guts of current cryptography and\n",
      "maybe a little history as well?  I read the piece in this month's WIRED, can\n",
      "anyone tell me how much I should trust the references they suggest?\n",
      "\n",
      "  Thanks in advance,\n",
      "   Steve.\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Have you read the applicable part of the Constitution and interpreted it IN \n",
      "CONTEXT?  If not, please do so before posting this misinterpretation again.\n",
      "It refers to the right of the people to organize a militia, not for individuals \n",
      "to carry handguns, grenades, and assault rifles.  \n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Steve I'm glad to see that you abandoned the preamble thing.  What; did\n",
      "you do a word search to find Welfare somewhere else in the constitution?\n",
      "\n",
      "[my comments and paraphrases in brackets]\n",
      "\n",
      "Article I Section 8:  [in some ways the guts of the constitution]\n",
      "\n",
      "The Congress shall have the Power:\n",
      "\n",
      "1. To lay and collect Taxes, Duties, Imposts and Excises, \n",
      "       to pay the Debts [indebtedness as defined in the document]\n",
      "       and provide for the common Defence [Defence as precisely defined]\n",
      "       and general Welfare [as defined through the document, mostly in ways\n",
      "                           [that limit the government.]\n",
      "         of the United States; [but the above taxes shall be uniform through-\n",
      "                               [out the U.S.]\n",
      "\n",
      "    [so far the congress has been given the power to collect taxes uniformly]\n",
      "\n",
      "    [ then ... ]\n",
      "\n",
      "2.  To borrow Money [...]\n",
      "3.  To regulate Commerce with foreign Nations, [interstate and Indian tribes]\n",
      "4.  To [do uniform Naturalization and Bankruptcies]\n",
      "5.  To coin Money, regulate the Value thereof, and [etc.]\n",
      "6.  To [punish counterfeiters]\n",
      "7.  To establish Post Offices and post Roads\n",
      "8.  To [provide patents and copyrights]\n",
      "9.  To constitute Tribunals inferior to the Supreme Court;\n",
      "10. To define and punish Piracies and [etc.]\n",
      "11. To declare War, [and etc.]\n",
      "12. To raise and support Armies,[but for no longer than two years at a stretch]\n",
      "13. To provide and maintain a Navy, [notice no time limit on this one]\n",
      "14. To [make the rules for the army and navy]\n",
      "15. To provide for calling forth the Militia to execute the Laws [etc.]\n",
      "16. To provide for [training of the army except for some state stuff]\n",
      "17. To exercise exclusive Legislation [in D.C]\n",
      "18. To make all Laws [necessary to execute the foregoing \"Powers\"].\n",
      "\n",
      "Your original instinct was right.  Looking to other nations for precedents\n",
      "that support an expansive liberal agenda is much easier than looking to the\n",
      "slim pickins found in the constitution.\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "\"We hold these truths to be self-evident, that all men are created\n",
      "equal, that they are endowed by their Creator with certain unalienable\n",
      "Rights, that among these are Life, Liberty, and the pursuit of Happiness.\"\n",
      "\n",
      "\t\t\t\tDeclaration of Independence\n",
      "\t\t\t\t\t4 July 1776\n",
      "--------------FIN DOCUMENTO------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Construir el contexto de forma estructurada\n",
    "# En lugar de concatenar el texto directamente, lo separo\n",
    "# Esto ayuda al LLM a entender que son fragmentos distintos\n",
    "context_separator = \"\\n\\n---\\n\\n\"\n",
    "retrieved_docs = [newsgroupsdocs[idx] for idx in top_results.indices]\n",
    "context = context_separator.join(retrieved_docs)\n",
    "\n",
    "print(\"--- CONTEXTO RECUPERADO ---\")\n",
    "print(context)\n",
    "print(\"--------------FIN DOCUMENTO------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b0f377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Diseño del Prompt\n",
    "\n",
    "# SÍNTESIS DEL CONTEXTO\n",
    "# En este paso, condensamos todos los fragmentos de texto recuperados en un único\n",
    "# párrafo coherente. Esto limpia el \"ruido\" y facilita el trabajo para la recuperación de información\n",
    "response = client.chat.completions.create(\n",
    "model=\"gpt-4o\",\n",
    "messages=[\n",
    "{\n",
    "\"role\": \"system\",\n",
    "\"content\": \"\"\"\n",
    " Eres un asistente experto en síntesis de información. Tu trabajo es responder a la pregunta del usuario resumiendo los puntos clave encontrados en un conjunto de documentos de texto.\n",
    " Reglas:\n",
    " 1. Lee la pregunta general del usuario.\n",
    " 2. Examina TODOS los documentos proporcionados en el contexto.\n",
    " 3. En lugar de buscar una respuesta directa, tu objetivo es construir un resumen coherente sobre el tema de la pregunta basándote en los diferentes puntos mencionados en los textos.\n",
    " 4. Basa tu respuesta 100% en la información de los documentos. No inventes nada.\n",
    " 5. Si los documentos no contienen absolutamente ninguna mención relevante al tema de la pregunta, responde únicamente: \"No se encontraron menciones relevantes en el corpus.\"\n",
    " \"\"\"\n",
    "         },\n",
    "         {\n",
    "             \"role\": \"user\",\n",
    "             \"content\": f\"\"\"\n",
    " PREGUNTA DEL USUARIO:\n",
    " \"{query}\"\n",
    "\n",
    " ---\n",
    "DOCUMENTOS DE CONTEXTO PARA SINTETIZAR:\n",
    " {context}\n",
    " ---\n",
    "\n",
    "ahora, resume lo que estos documentos dicen sobre el tema de la pregunta del usuario.\n",
    "\"\"\"\n",
    "}\n",
    "],\n",
    "temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e58bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- La respuesta de la pregunta -- What this document explain about United States? -- es:  \n",
      "\n",
      "Los documentos proporcionados no abordan directamente el tema de los Estados Unidos de manera exhaustiva, pero hay algunas menciones relevantes:\n",
      "\n",
      "1. **Gasto en el Medio Oriente**: Se menciona que Estados Unidos gasta miles de millones de dólares en ayuda económica y militar a las partes en conflicto en el Medio Oriente, específicamente en el contexto del conflicto israelí-palestino. Además, se sugiere que una parte de estos fondos podría usarse para fomentar matrimonios mixtos entre israelíes y palestinos como una forma de promover la paz.\n",
      "\n",
      "2. **Constitución de los Estados Unidos**: Hay una discusión sobre la interpretación de la Constitución de los Estados Unidos, específicamente sobre el derecho a portar armas y el papel del Congreso en la recaudación de impuestos y otras funciones gubernamentales. Se destaca el poder del Congreso para regular el comercio, establecer oficinas de correos, declarar la guerra, y otras responsabilidades definidas en el Artículo I, Sección 8 de la Constitución.\n",
      "\n",
      "3. **Declaración de Independencia**: Se cita la famosa frase de la Declaración de Independencia que afirma que \"todos los hombres son creados iguales\" y que tienen derechos inalienables como la vida, la libertad y la búsqueda de la felicidad.\n",
      "\n",
      "Estas menciones reflejan aspectos de la política exterior y la estructura constitucional de los Estados Unidos, pero no ofrecen una visión completa o detallada sobre el país en su totalidad.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- La respuesta de la pregunta --\", query, \"-- es:  \\n\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8af71e",
   "metadata": {},
   "source": [
    "## Parte 2, ejemplo de consulta con embeddings utilizando contexto del resultado de los documentos mas similares por similitud del coseno en embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aeeb83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install PyMuPDF\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ccaae7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de páginas del PDF: 581\n"
     ]
    }
   ],
   "source": [
    "# 1: Cargar y procesar el PDF\n",
    "pdf_path = 'irbook.pdf'\n",
    "doc = fitz.open(pdf_path)\n",
    "# Extraer todo el texto del PDF\n",
    "full_text = \"\"\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    full_text += page.get_text() + \"\\n\"\n",
    "\n",
    "print(f\"Total de páginas del PDF: {len(doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba366b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Limpieza básica del texto \n",
    "# Patrones específicos a eliminar\n",
    "patterns_to_remove = [\n",
    "    r'Online edition \\(c\\) \\d{4} Cambridge UP',\n",
    "    r'DRAFT!.*Feedback welcome\\.',\n",
    "    r'Cambridge University Press\\. Feedback welcome\\.',\n",
    "    r'Figure \\d+\\.\\d+:.*',\n",
    "    r'Table \\d+\\.\\d+:.*',\n",
    "    r'\\n\\d+\\n'  # Números de página\n",
    "]\n",
    "clean_text = full_text\n",
    "for pattern in patterns_to_remove:\n",
    "    clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE)\n",
    "# Normalizar espacios\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93e7563a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks creados: 2489\n"
     ]
    }
   ],
   "source": [
    "# 3: Crear chunks semánticos - heuristica \n",
    "sentences = sent_tokenize(clean_text)\n",
    "# Agrupar oraciones en chunks de 5 oraciones cada uno\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "chunk_size = 5  # Número de oraciones por chunk\n",
    "for i, sentence in enumerate(sentences):\n",
    "    current_chunk.append(sentence)\n",
    "    if len(current_chunk) >= chunk_size:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = []\n",
    "# Añadir el último chunk si tiene contenido\n",
    "if current_chunk:\n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "print(f\"Total chunks creados: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb5535bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks válidos: 2473\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>Online edition (c) 2009 Cambridge UP An Introd...</td>\n",
       "      <td>online edition c cambridge introduction inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>The unrounded values are: 806,791 documents, 2...</td>\n",
       "      <td>unrounded value document token per document di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>We performed stemming with the Porter stemmer ...</td>\n",
       "      <td>performed stemming porter stemmer chapter page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>Commas in γ codes are for readability only and...</td>\n",
       "      <td>comma γ code readability part actual index dic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>So for X2 &gt; 6.63 the assumption of independenc...</td>\n",
       "      <td>assumption independence rejected ten largest c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>(2001) Zhai: Lafferty and Zhai (2001), Laffert...</td>\n",
       "      <td>zhai lafferty zhai lafferty zhai tao et al zha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>(2001b) Zien: Chapelle et al. (2006) Zipf: Zip...</td>\n",
       "      <td>zien chapelle et al zipf zipf ziviani badue et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>(2002), Heinz and Zobel (2003), Heinz et al. (...</td>\n",
       "      <td>heinz zobel heinz et al kaszkiel zobel lester ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>(2002), Williams and Zobel (2005), Williams et...</td>\n",
       "      <td>williams zobel williams et al zobel zobel dart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...</td>\n",
       "      <td>(2006) del Bimbo: del Bimbo (1999) Online edit...</td>\n",
       "      <td>del bimbo del bimbo online edition c cambridge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2473 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    raw  \\\n",
       "0     Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "1     Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "2     Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "3     Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "4     Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "...                                                 ...   \n",
       "2484  Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "2485  Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "2486  Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "2487  Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "2488  Online edition (c)\\n2009 Cambridge UP\\nAn\\nInt...   \n",
       "\n",
       "                                             clean_text  \\\n",
       "0     Online edition (c) 2009 Cambridge UP An Introd...   \n",
       "1     The unrounded values are: 806,791 documents, 2...   \n",
       "2     We performed stemming with the Porter stemmer ...   \n",
       "3     Commas in γ codes are for readability only and...   \n",
       "4     So for X2 > 6.63 the assumption of independenc...   \n",
       "...                                                 ...   \n",
       "2484  (2001) Zhai: Lafferty and Zhai (2001), Laffert...   \n",
       "2485  (2001b) Zien: Chapelle et al. (2006) Zipf: Zip...   \n",
       "2486  (2002), Heinz and Zobel (2003), Heinz et al. (...   \n",
       "2487  (2002), Williams and Zobel (2005), Williams et...   \n",
       "2488  (2006) del Bimbo: del Bimbo (1999) Online edit...   \n",
       "\n",
       "                                         processed_text  \n",
       "0     online edition c cambridge introduction inform...  \n",
       "1     unrounded value document token per document di...  \n",
       "2     performed stemming porter stemmer chapter page...  \n",
       "3     comma γ code readability part actual index dic...  \n",
       "4     assumption independence rejected ten largest c...  \n",
       "...                                                 ...  \n",
       "2484  zhai lafferty zhai lafferty zhai tao et al zha...  \n",
       "2485  zien chapelle et al zipf zipf ziviani badue et...  \n",
       "2486  heinz zobel heinz et al kaszkiel zobel lester ...  \n",
       "2487  williams zobel williams et al zobel zobel dart...  \n",
       "2488  del bimbo del bimbo online edition c cambridge...  \n",
       "\n",
       "[2473 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4: Preprocesamiento con NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Preprocesar cada chunk\n",
    "processed_chunks = []\n",
    "for chunk in chunks:\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(chunk.lower())\n",
    "    # Filtrar y lematizar\n",
    "    filtered_tokens = [\n",
    "        lemmatizer.lemmatize(token) \n",
    "        for token in tokens \n",
    "        if token.isalpha() and token not in stop_words\n",
    "    ]\n",
    "    processed_chunks.append(\" \".join(filtered_tokens))\n",
    "# Crear DataFrame\n",
    "pdf_df = pd.DataFrame({'raw': full_text,\n",
    "    'clean_text': chunks,\n",
    "    'processed_text': processed_chunks\n",
    "})\n",
    "# Filtrar chunks vacíos\n",
    "pdf_df = pdf_df[pdf_df['processed_text'].str.len() > 10]\n",
    "print(f\"Chunks válidos: {len(pdf_df)}\")\n",
    "pdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fd075d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 78/78 [00:21<00:00,  3.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 5: Generar embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(\n",
    "    pdf_df['processed_text'].tolist(), \n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf37e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Procesar consulta\n",
    "query = \"¿Qué es el modelo de espacio vectorial (vector space model) y en qué capítulo se explica?\"\n",
    "#query = \"Who is Cristobal Colon based to the documents?\"\n",
    "# Preprocesar la consulta\n",
    "query_tokens = word_tokenize(query.lower())\n",
    "processed_query_tokens = [\n",
    "    lemmatizer.lemmatize(token) \n",
    "    for token in query_tokens \n",
    "    if token.isalpha() and token not in stop_words\n",
    "]\n",
    "processed_query = \" \".join(processed_query_tokens)\n",
    "\n",
    "# Generar embedding para la consulta\n",
    "query_embedding = embedding_model.encode(processed_query, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84c91692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1009, 0.2660, 0.1792,  ..., 0.0837, 0.1768, 0.0811])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcular similitud coseno\n",
    "cos_scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86c61601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.4825, 0.4611, 0.4603, 0.4521, 0.4456]),\n",
       "indices=tensor([2026, 2162, 2154, 1915, 1828]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener los 5 chunks más relevantes\n",
    "top_k = 5\n",
    "top_results = torch.topk(cos_scores, k=top_k)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae4bc4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunks más relevantes:\n",
      "Chunk 1 (similitud: 0.4825):\n",
      "URL: citeseer.ist.psu.edu/article/kumar00web.html. 441, 526, 529, 531,Kupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A trainable document sum- marizer. In Proc....\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (similitud: 0.4611):\n",
      "The author-topic model for authors and documents. In Proc. UAI, pp. 487–494. 418, 523, 530, 531 Ross, Sheldon....\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (similitud: 0.4603):\n",
      "In Proc. WWW, pp. 707–715. 348, 520, 529 Online edition (c) 2009 Cambridge UP BibliographyRiezler, Stefan, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007....\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 (similitud: 0.4521):\n",
      "373, 522, 523 Han, Eui-Hong, and George Karypis. 2000. Centroid-based document classiﬁcation: Analysis and experimental results. In Proc. PKDD, pp....\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5 (similitud: 0.4456):\n",
      "1998. Is this document relevant? . . ....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Construir contexto con los chunks más relevantes\n",
    "context_parts = []\n",
    "print(\"\\nChunks más relevantes:\")\n",
    "for i, (score, idx) in enumerate(zip(top_results.values, top_results.indices)):\n",
    "    chunk_idx = idx.item()\n",
    "    print(f\"Chunk {i+1} (similitud: {score:.4f}):\")\n",
    "    print(pdf_df.iloc[chunk_idx]['clean_text'][:500] + \"...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    context_parts.append(\n",
    "        f\"FRAGMENTO {i+1} (Relevancia: {score:.2f}):\\n\"\n",
    "        f\"{pdf_df.iloc[chunk_idx]['clean_text']}\\n\"\n",
    "    )\n",
    "context = \"\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41410ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7: Construir y enviar prompt\n",
    "prompt = f\"\"\"\n",
    "Eres un experto en Recuperación de Información. Basado EXCLUSIVAMENTE en los siguientes fragmentos del libro, \n",
    "responde la pregunta del usuario. Si la información no está en los fragmentos, di claramente que no tienes datos suficientes.\n",
    "\n",
    "PREGUNTA: {query}\n",
    "\n",
    "FRAGMENTOS RELEVANTES \"Contexto\":\n",
    "{context}\n",
    "\n",
    "INSTRUCCIONES:\n",
    "1. Responde en español\n",
    "2. Menciona el capítulo o sección relevante si aparece en los fragmentos\n",
    "3. Si la información está incompleta, sugiere buscar en capítulos específicos\n",
    "4. Sé preciso y conciso\n",
    "\n",
    "RESPUESTA:\n",
    "\"\"\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente académico especializado en Recuperación de Información.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431eaa3b",
   "metadata": {},
   "source": [
    "## RESULTADO PARTE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d025b084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta a la query  Who is Enrique Mafla based to the documents?  :\n",
      "================================================================================\n",
      "No tengo datos suficientes en los fragmentos proporcionados para determinar quién es Enrique Mafla. Te sugiero buscar en capítulos o secciones adicionales del libro que puedan contener información sobre él.\n"
     ]
    }
   ],
   "source": [
    "# 8: Mostrar resultados\n",
    "print(\"Respuesta a la query \", query , \" :\")\n",
    "print(\"=\" * 80)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16dc370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fragmentos utilizados en el contexto:\n",
      "\n",
      "Fragmento 1:\n",
      "FRAGMENTO 1 (Relevancia: 0.41):\n",
      "3. In Section 6.3 we show that by viewing each document as a vector of such weights, we can compute a score between a query and each document. This view is known as vector space scoring. Section 6.4 develops several variants of term-weighting for the vector space model. Chapter 7 develops computational aspects of vector space scoring, and related topics.\n",
      "...\n",
      "\n",
      "\n",
      "Fragmento 2:\n",
      "FRAGMENTO 2 (Relevancia: 0.40):\n",
      "The representa- tion of a set of documents as vectors in a common vector space is known as the vector space model and is fundamental to a host of information retrieval op- VECTOR SPACE MODEL erations ranging from scoring documents on a query, document classiﬁcation and document clustering. We ﬁrst develop the basic ideas underlying vector space scoring; a pivotal step in this development is the view (Section 6.3.2) of queries as vectors in the same vector space as...\n",
      "\n",
      "\n",
      "Fragmento 3:\n",
      "FRAGMENTO 3 (Relevancia: 0.37):\n",
      "We can- Online edition (c) 2009 Cambridge UP14 Vector space classiﬁcation mode time complexity training Θ(|D|Lave + |C||V|) testing Θ(La + |C|Ma) = Θ(|C|Ma) ◮Table 14.2 Training and test times for Rocchio classiﬁcation. Lave is the average number of tokens per document. La and Ma are the numbers of tokens and types, respectively, in the test document. Computing Euclidean distance between the class centroids and a document is Θ(|C|Ma). not represent the “a” class w...\n",
      "\n",
      "\n",
      "Fragmento 4:\n",
      "FRAGMENTO 4 (Relevancia: 0.37):\n",
      ". , e|V|⟩∈{0, 1}|V|. In this chapter we adopt a different representation for text classiﬁcation, the vector space model, developed in Chapter 6. It represents each document as a vector with one real-valued com- ponent, usually a tf-idf weight, for each term. Thus, the document space X, the domain of the classiﬁcation function γ, is R|V|.\n",
      "...\n",
      "\n",
      "\n",
      "Fragmento 5:\n",
      "FRAGMENTO 5 (Relevancia: 0.35):\n",
      "The bias-variance tradeoff provides insight into their success. Typical classes in text classiﬁcation are complex and seem unlikely to be modeled well linearly. However, this intuition is misleading for the high-dimensional spaces that we typically encounter in text appli- cations. With increased dimensionality, the likelihood of linear separability increases rapidly (Exercise 14.17). Thus, linear models in high-dimensional spaces are quite powerful despite their ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los fragmentos usados\n",
    "print(\"\\nFragmentos utilizados en el contexto:\")\n",
    "for i, part in enumerate(context_parts):\n",
    "    print(f\"\\nFragmento {i+1}:\")\n",
    "    print(part[:500] + \"...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
