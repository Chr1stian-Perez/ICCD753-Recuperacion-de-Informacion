{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f77189596d7c95",
   "metadata": {},
   "source": [
    "# Ejercicio 11 : Asistente RAG Conversacional\n",
    "\n",
    "## Objetivo de la práctica\n",
    "\n",
    "Construir un asistente que:\n",
    "\n",
    "1. Recibe una pregunta del usuario\n",
    "2. Recupera texto relevante de un corpus (ej. libro de Baeza-Yates)\n",
    "3. Genera una respuesta basada en los documentos encontrados\n",
    "4. Mantiene el historial de conversación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756d113e9d3deff",
   "metadata": {},
   "source": [
    "## Parte 0: Librerías necesarias\n",
    "- openai\n",
    "- faiss-cpu\n",
    "- sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "#!pip install google-generativeai \"faiss-cpu\" \"sentence-transformers\" pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e1072aca40b2",
   "metadata": {},
   "source": [
    "## Parte 1: Carga del corpus\n",
    "\n",
    "Aquí se debe cargar el corpus con los documentos en PDF.\n",
    "\n",
    "- Libro de Stanford\n",
    "- Libro BM25\n",
    "- Paper: Marcia Bates (1989). The design of browsing and berrypicking techniques for the online search interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8d90af6f06d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Procesado 'Libro_BM25.pdf' con 59 páginas.\n",
      "  - Procesado 'Libro_Stanford.pdf' con 581 páginas.\n",
      "  - Procesado 'Paper_Marcia_Bates.pdf' con 18 páginas.\n",
      "Carga finalizada. Total de páginas extraídas: 658\n",
      "\n",
      "Ejemplo de una página cargada:\n",
      "{'source': 'Libro_BM25.pdf', 'page': 11, 'content': '2.4 Some Notation\\n341\\n2.4.2\\nDetails\\nWe start with the probability of relevance of a given document and\\nquery.\\nThe ﬁrst three steps are simple algebraic transformations. In\\nStep (2.1), we simply replace the probability by an odds-ratio.2 In\\nStep (2.2), we perform Bayesian inversions on both numerator and\\ndenominator. In Step (2.3), we drop the second component which is\\nindependent of the document, and therefore does not aﬀect the rank-\\ning of the documents.\\nIn Step (2.4) (term independence assumption), we expand each\\nprobability as a product over the terms of the vocabulary. This step\\ndepends on an assumption of statistical independence between terms —\\nactually a pair of such assumptions, for the relevant and non-relevant\\ncases, respectively. Note that we are not assuming unconditional term\\nindependence, but a weaker form, namely conditional independence.3\\nThis is clearly a much more arguable step: in general terms will not be\\nstatistically independent in this fashion. The obvious reason for taking\\nthis step is to lead us to a simple and tractable (even if approximate)\\nscoring function. However, we may make some further arguments to\\nsupport this step:\\n1. Models of this type in other domains, known as Na¨ıve Bayes\\nmodels, are well known to be remarkably good, simple and\\nrobust, despite signiﬁcant deviations from independence.\\nExperimental evidence in IR provides some support for this\\ngeneral statement.\\n2. The pair of assumptions is not in general equivalent to\\na blanket assumption of independence between terms over\\nthe whole collection. On the contrary, for a pair of query\\nterms, both statistically correlated with relevance, the pair\\nof assumptions predict a positive association between the\\ntwo terms over the whole collection. In fact we often observe\\nsuch a positive association. In eﬀect the model says that this\\n2 This transformation is order preserving; the odds-ratio of p is\\np\\n1−p , and this function is a\\nmonotonous increasing function of p ∈[0,1).\\n3 P(ab|c) = P(a|c)P(b|c).\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Carga de los documentos PDF\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "# Nombres de los archivos PDF que servirán como corpus\n",
    "pdf_files = [\"Libro_BM25.pdf\", \"Libro_Stanford.pdf\", \"Paper_Marcia_Bates.pdf\"]\n",
    "\n",
    "def load_corpus(file_paths):\n",
    "    \"\"\"\n",
    "    Carga el texto y metadatos de una lista de archivos PDF.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): Lista de rutas a los archivos PDF.\n",
    "        \n",
    "    Returns:\n",
    "        list: Una lista de diccionarios, cada uno con el nombre del archivo,\n",
    "              el número de página y el contenido de esa página.\n",
    "    \"\"\"\n",
    "    corpus_pages = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            doc = fitz.open(path)\n",
    "            for page_num, page in enumerate(doc):\n",
    "                corpus_pages.append({\n",
    "                    \"source\": os.path.basename(path),\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"content\": page.get_text()\n",
    "                })\n",
    "            print(f\"  - Procesado '{os.path.basename(path)}' con {len(doc)} páginas.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando el archivo {path}: {e}\")\n",
    "            \n",
    "    print(f\"Carga finalizada. Total de páginas extraídas: {len(corpus_pages)}\")\n",
    "    return corpus_pages\n",
    "corpus_documents = load_corpus(pdf_files)\n",
    "\n",
    "# Imprimir un ejemplo de una página cargada\n",
    "print(\"\\nEjemplo de una página cargada:\")\n",
    "print(corpus_documents[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333f3666484f4ce",
   "metadata": {},
   "source": [
    "## Parte 2: Procesamiento del Corpus\n",
    "\n",
    "Aquí se debe obtener el corpus procesado. El corpus estará formado por documentos que corresponden a las secciones (o subsecciones) de los libros. Cada documento debe indicar a qué libro corresponde, así como las páginas en las que está dentro de ese libro.\n",
    "\n",
    "Recuerden que los documentos procesados no deben contener textos o caracteres ajenos al tema del que tratan.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fde136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de NLTK y función de preprocesamiento\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb82f4c1d92ede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentación finalizada. Número total de chunks: 3453\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento del texto\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Limpia y normaliza el texto de forma menos agresiva.\"\"\"\n",
    "    # 1. Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # 2. Eliminar solo caracteres especiales no deseados, pero conservar números, puntos y guiones.\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.\\-]', '', text)\n",
    "    # 3. Tokenizar\n",
    "    tokens = word_tokenize(text)\n",
    "    # 4. Eliminar stopwords y lematizar\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(processed_tokens)\n",
    "# Segmentación del corpus aplicando la limpieza\n",
    "def process_and_chunk_corpus(documents, chunk_size=512, chunk_overlap=25):\n",
    "    \"\"\"\n",
    "    Segmenta los documentos y aplica el preprocesamiento, guardando tanto el \n",
    "    chunk original como el procesado.\n",
    "    \"\"\"\n",
    "    document_chunks = []\n",
    "    for doc in documents:\n",
    "        raw_content = doc[\"content\"].replace('\\n', ' ').strip()\n",
    "        if not raw_content:\n",
    "            continue\n",
    "        start = 0\n",
    "        while start < len(raw_content):\n",
    "            end = start + chunk_size\n",
    "            raw_chunk = raw_content[start:end]\n",
    "            processed_chunk_text = preprocess_text(raw_chunk)\n",
    "            \n",
    "            if processed_chunk_text:\n",
    "                document_chunks.append({\n",
    "                    \"source\": doc[\"source\"],\n",
    "                    \"page\": doc[\"page\"],\n",
    "                    \"raw_content\": raw_chunk,\n",
    "                    \"processed_content\": processed_chunk_text\n",
    "                })\n",
    "            start += chunk_size - chunk_overlap\n",
    "            \n",
    "    print(f\"Segmentación finalizada. Número total de chunks: {len(document_chunks)}\")\n",
    "    return document_chunks\n",
    "\n",
    "document_chunks = process_and_chunk_corpus(corpus_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0264565d37ac35",
   "metadata": {},
   "source": [
    "## Parte 3: Cálculo de Embeddings e Indexación en base de datos vectorial\n",
    "\n",
    "Aquí, una vez que se ha calculado el embedding de cada documento, se deberá indexar este embedding en una base de datos vectorial como FAISS, ChromaDB o Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f22cae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Creación de embeddings e indexación\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8325a0116779dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 108/108 [00:36<00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice FAISS construido. Total de vectores indexados: 3453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generación de embeddings con SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "# Usamos el contenido preprocesado para generar los embeddings\n",
    "corpus_embeddings = model.encode(\n",
    "    [chunk['processed_content'] for chunk in document_chunks],\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "# Construir el índice FAISS\n",
    "embedding_dim = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(corpus_embeddings)\n",
    "print(f\"Índice FAISS construido. Total de vectores indexados: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02916370ab201c1",
   "metadata": {},
   "source": [
    "## Parte 4: Búsqueda y obtención del contexto\n",
    "\n",
    "En esta parte debemos definir una _query_ y buscar los documentos que más se relacionan con ella.\n",
    "\n",
    "Estos documentos formarán el contexto que vamos a entregar al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccd438b201b197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de búsqueda semántica\n",
    "def search_context(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Busca los chunks más relevantes para una query y retorna su contenido original.\n",
    "    \"\"\"\n",
    "    print(f\"\\n Buscando contexto para la query: '{query}'\")\n",
    "    # Preprocesar la query del usuario de la misma manera que el corpus\n",
    "    processed_query = preprocess_text(query)\n",
    "    # Codificar la query procesada\n",
    "    query_embedding = model.encode(processed_query, convert_to_numpy=True).reshape(1, -1)\n",
    "    # Buscar en FAISS\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    # Recuperar los chunks y formatear el contexto para el LLM\n",
    "    context = \"\"\n",
    "    retrieved_metadata = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        chunk = document_chunks[idx]\n",
    "        context += f\"Fuente {i+1} (Archivo: {chunk['source']}, Página: {chunk['page']}):\\n\"\n",
    "        # Usamos el contenido original (raw) para el prompt del LLM\n",
    "        context += f\"\\\"{chunk['raw_content']}\\\"\\n\\n\"\n",
    "        retrieved_metadata.append(chunk)\n",
    "    print(\"Contexto recuperado exitosamente\")\n",
    "    return context, retrieved_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158cb4c44f6966f",
   "metadata": {},
   "source": [
    "## Parte 5: Generación de Respuesta\n",
    "\n",
    "Aquí, entregamos el contexto al LLM, y él nos responde a la _query_ con una explicación en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e15139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de respuesta con Gemini y gestión de la conversación\n",
    "import google.generativeai as genai\n",
    "import getpass\n",
    "\n",
    "GEMINI_API_KEY = \"API\"\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = genai.GenerativeModel('gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2710999824ea677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecanismo de mantenimiento conversacional \"Contexto\"\n",
    "conversation_history = []\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "def generate_answer_with_chat(query):\n",
    "    \"\"\"Gestión de contexto y generación de la respuesta con Gemini\"\"\"\n",
    "    global conversation_history\n",
    "    \n",
    "    history_str = \"\\n\".join([f\"Usuario: {q}\\nAsistente: {a}\" for q, a in conversation_history])\n",
    "    contextualized_query = f\"Historial de la conversación:\\n{history_str}\\nConsulta actual del usuario: {query}\"\n",
    "    \n",
    "    context, metadata = search_context(contextualized_query, top_k=15)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    *Rol y Objetivo:* Eres un asistente experto en Recuperación de Información. Tu objetivo no es solo encontrar información, sino sintetizar el conocimiento contenido en los fragmentos de texto proporcionados para dar una respuesta completa y bien fundamentada a la \"Consulta actual del usuario\".\n",
    "\n",
    "    Instrucciones Clave:\n",
    "    1.  *Analiza y Sintetiza: Lee y comprende todos los fragmentos en el \"Contexto de documentos\". No te limites a un solo fragmento. Conecta ideas de diferentes fuentes para construir la mejor respuesta posible.\n",
    "    2.  *Infiere si es necesario: Si la respuesta no está explícitamente declarada pero puede ser razonablemente deducida a partir de la información disponible, hazlo.\n",
    "    3.  *Cita tus fuentes: Tu respuesta DEBE estar fundamentada en el texto. Cita cada pieza de información que uses con el formato [Fuente X, Archivo: <nombre>, Página: <numero>].\n",
    "    4.  *Fallback Inteligente: Si después de analizar profundamente todo el contexto, la información simplemente no existe para responder la pregunta, indica amablemente que el tema parece estar fuera del alcance del conocimiento de los documentos que componen el corpus. Responde \"No tengo suficiente información para res´ponder la pregunta\" si no existe ninguna relación de la consulta del usuario con el contenido recuperado.\n",
    "\n",
    "    --- Contexto de documentos ---\n",
    "    {context}\n",
    "    --- Fin del Contexto ---\n",
    "    \n",
    "    {contextualized_query}\n",
    "    \n",
    "    **Asistente Experto:**\n",
    "    \"\"\"\n",
    "    if not gemini_model:\n",
    "        return \"El servicio de GEMINI no está disponible\", []\n",
    "\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        answer = response.text\n",
    "        \n",
    "        conversation_history.append((query, answer))\n",
    "        if len(conversation_history) > WINDOW_SIZE:\n",
    "            conversation_history.pop(0)\n",
    "            \n",
    "        return answer, metadata\n",
    "    except Exception as e:\n",
    "        return f\"Error al generar la respuesta con Gemini: {e}\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f84a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- INICIANDO ASISTENTE RAG CONVERSACIONAL CON GEMINI ---\n",
      "Escribe 'salir' para terminar la conversación.\n",
      "\n",
      "\n",
      " Buscando contexto para la query: 'Historial de la conversación:\n",
      "\n",
      "Consulta actual del usuario: Quien es Marcia Bates?'\n",
      "Contexto recuperado exitosamente\n",
      "\n",
      "----------------\n",
      "\n",
      "Asistente:\n",
      "Marcia Bates es una autora que ha publicado trabajos en el campo de la recuperación de información y la bibliografía. Entre sus publicaciones se encuentran:\n",
      "\n",
      "*   \"The Fallacy of the Perfect 30-Item Online Search\", publicada en RQ, 24, 1, 1984, pp. 43-50 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "*   \"Rigorous Systematic Bibliography\", publicada en RQ, 16, 1, 1976, pp. 7-26 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "\n",
      "----------------\n",
      "\n",
      " Buscando contexto para la query: 'Historial de la conversación:\n",
      "Usuario: Quien es Marcia Bates?\n",
      "Asistente: Marcia Bates es una autora que ha publicado trabajos en el campo de la recuperación de información y la bibliografía. Entre sus publicaciones se encuentran:\n",
      "\n",
      "*   \"The Fallacy of the Perfect 30-Item Online Search\", publicada en RQ, 24, 1, 1984, pp. 43-50 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "*   \"Rigorous Systematic Bibliography\", publicada en RQ, 16, 1, 1976, pp. 7-26 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "Consulta actual del usuario: Que es la busqueda booleana?}'\n",
      "Contexto recuperado exitosamente\n",
      "\n",
      "----------------\n",
      "\n",
      "Asistente:\n",
      "No tengo suficiente información para responder la pregunta sobre qué es la búsqueda booleana. Si bien se menciona el término \"matriz de coocurrencia booleana\" en el contexto de un ejercicio relacionado con la recuperación de información [Fuente 6, Archivo: Libro_Stanford.pdf, Página: 230; Fuente 14, Archivo: Libro_Stanford.pdf, Página: 230], los documentos proporcionados no contienen una definición o explicación de lo que constituye una búsqueda booleana o una matriz booleana.\n",
      "\n",
      "----------------\n",
      "\n",
      " Buscando contexto para la query: 'Historial de la conversación:\n",
      "Usuario: Quien es Marcia Bates?\n",
      "Asistente: Marcia Bates es una autora que ha publicado trabajos en el campo de la recuperación de información y la bibliografía. Entre sus publicaciones se encuentran:\n",
      "\n",
      "*   \"The Fallacy of the Perfect 30-Item Online Search\", publicada en RQ, 24, 1, 1984, pp. 43-50 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "*   \"Rigorous Systematic Bibliography\", publicada en RQ, 16, 1, 1976, pp. 7-26 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "Usuario: Que es la busqueda booleana?}\n",
      "Asistente: No tengo suficiente información para responder la pregunta sobre qué es la búsqueda booleana. Si bien se menciona el término \"matriz de coocurrencia booleana\" en el contexto de un ejercicio relacionado con la recuperación de información [Fuente 6, Archivo: Libro_Stanford.pdf, Página: 230; Fuente 14, Archivo: Libro_Stanford.pdf, Página: 230], los documentos proporcionados no contienen una definición o explicación de lo que constituye una búsqueda booleana o una matriz booleana.\n",
      "Consulta actual del usuario: como es el proceso de Obtaining the character sequence in a document?'\n",
      "Contexto recuperado exitosamente\n",
      "\n",
      "----------------\n",
      "\n",
      "Asistente:\n",
      "No tengo suficiente información para responder la pregunta sobre el proceso de obtención de la secuencia de caracteres en un documento. Los documentos proporcionados se centran en temas como la recuperación de información, sistemas de búsqueda booleana, la confianza en el contenido web, y trabajos específicos de autores como Marcia Bates, pero no describen el proceso técnico de cómo se obtiene o procesa la secuencia de caracteres de un documento.\n",
      "\n",
      "----------------\n",
      "\n",
      " Buscando contexto para la query: 'Historial de la conversación:\n",
      "Usuario: Quien es Marcia Bates?\n",
      "Asistente: Marcia Bates es una autora que ha publicado trabajos en el campo de la recuperación de información y la bibliografía. Entre sus publicaciones se encuentran:\n",
      "\n",
      "*   \"The Fallacy of the Perfect 30-Item Online Search\", publicada en RQ, 24, 1, 1984, pp. 43-50 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "*   \"Rigorous Systematic Bibliography\", publicada en RQ, 16, 1, 1976, pp. 7-26 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "Usuario: Que es la busqueda booleana?}\n",
      "Asistente: No tengo suficiente información para responder la pregunta sobre qué es la búsqueda booleana. Si bien se menciona el término \"matriz de coocurrencia booleana\" en el contexto de un ejercicio relacionado con la recuperación de información [Fuente 6, Archivo: Libro_Stanford.pdf, Página: 230; Fuente 14, Archivo: Libro_Stanford.pdf, Página: 230], los documentos proporcionados no contienen una definición o explicación de lo que constituye una búsqueda booleana o una matriz booleana.\n",
      "Usuario: como es el proceso de Obtaining the character sequence in a document?\n",
      "Asistente: No tengo suficiente información para responder la pregunta sobre el proceso de obtención de la secuencia de caracteres en un documento. Los documentos proporcionados se centran en temas como la recuperación de información, sistemas de búsqueda booleana, la confianza en el contenido web, y trabajos específicos de autores como Marcia Bates, pero no describen el proceso técnico de cómo se obtiene o procesa la secuencia de caracteres de un documento.\n",
      "Consulta actual del usuario: Que son los sistemas de busqueda booleana?'\n",
      "Contexto recuperado exitosamente\n",
      "\n",
      "----------------\n",
      "\n",
      "Asistente:\n",
      "Los sistemas de búsqueda booleana se basan en consultas booleanas, que se caracterizan por ser muy precisas: un documento o coincide con la consulta o no lo hace [Fuente 10, Archivo: Libro_Stanford.pdf, Página: 52; Fuente 14, Archivo: Libro_Stanford.pdf, Página: 52]. Esta característica proporciona al usuario un mayor control y transparencia sobre los resultados de la recuperación [Fuente 10, Archivo: Libro_Stanford.pdf, Página: 52].\n",
      "\n",
      "En los inicios de los sistemas de recuperación de información, el interés comercial se inclinó rápidamente hacia los sistemas de recuperación booleana [Fuente 11, Archivo: Libro_Stanford.pdf, Página: 54].\n",
      "\n",
      "Incluso en la actualidad, en algunos dominios como el material legal, es posible un método eficaz de clasificación de documentos dentro de un modelo booleano. Por ejemplo, Westlaw, uno de los servicios comerciales de búsqueda legal más grandes, devuelve los documentos en orden cronológico inverso, lo cual es muy efectivo en la práctica [Fuente 10, Archivo: Libro_Stanford.pdf, Página: 52; Fuente 14, Archivo: Libro_Stanford.pdf, Página: 52]. Además, en 2007, la mayoría de los bibliotecarios jurídicos todavía recomendaban el uso de \"términos y conectores\" para búsquedas de alta recuperación, lo que se alinea con este tipo de sistemas [Fuente 10, Archivo: Libro_Stanford.pdf, Página: 52].\n",
      "\n",
      "Existe también un \"modelo booleano extendido\" que permite medir la cercanía de los términos dentro de un documento, limitando el número de palabras intervinientes o haciendo referencia a unidades estructurales como oraciones o párrafos [Fuente 14, Archivo: Libro_Stanford.pdf, Página: 52].\n",
      "\n",
      "----------------\n",
      "\n",
      " Buscando contexto para la query: 'Historial de la conversación:\n",
      "Usuario: Quien es Marcia Bates?\n",
      "Asistente: Marcia Bates es una autora que ha publicado trabajos en el campo de la recuperación de información y la bibliografía. Entre sus publicaciones se encuentran:\n",
      "\n",
      "*   \"The Fallacy of the Perfect 30-Item Online Search\", publicada en RQ, 24, 1, 1984, pp. 43-50 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "*   \"Rigorous Systematic Bibliography\", publicada en RQ, 16, 1, 1976, pp. 7-26 [Fuente 10, Archivo: Paper_Marcia_Bates.pdf, Página: 17].\n",
      "Usuario: Que es la busqueda booleana?}\n",
      "Asistente: No tengo suficiente información para responder la pregunta sobre qué es la búsqueda booleana. Si bien se menciona el término \"matriz de coocurrencia booleana\" en el contexto de un ejercicio relacionado con la recuperación de información [Fuente 6, Archivo: Libro_Stanford.pdf, Página: 230; Fuente 14, Archivo: Libro_Stanford.pdf, Página: 230], los documentos proporcionados no contienen una definición o explicación de lo que constituye una búsqueda booleana o una matriz booleana.\n",
      "Usuario: como es el proceso de Obtaining the character sequence in a document?\n",
      "Asistente: No tengo suficiente información para responder la pregunta sobre el proceso de obtención de la secuencia de caracteres en un documento. Los documentos proporcionados se centran en temas como la recuperación de información, sistemas de búsqueda booleana, la confianza en el contenido web, y trabajos específicos de autores como Marcia Bates, pero no describen el proceso técnico de cómo se obtiene o procesa la secuencia de caracteres de un documento.\n",
      "Usuario: Que son los sistemas de busqueda booleana?\n",
      "Asistente: Los sistemas de búsqueda booleana se basan en consultas booleanas, que se caracterizan por ser muy precisas: un documento o coincide con la consulta o no lo hace [Fuente 10, Archivo: Libro_Stanford.pdf, Página: 52; Fuente 14, Archivo: Libro_Stanford.pdf, Página: 52]. Esta característica proporciona al usuario un mayor control y transparencia sobre los resultados de la recuperación [Fuente 10, Archivo: Libro_Stanford.pdf, Página: 52].\n",
      "\n",
      "En los inicios de los sistemas de recuperación de información, el interés comercial se inclinó rápidamente hacia los sistemas de recuperación booleana [Fuente 11, Archivo: Libro_Stanford.pdf, Página: 54].\n",
      "\n",
      "Incluso en la actualidad, en algunos dominios como el material legal, es posible un método eficaz de clasificación de documentos dentro de un modelo booleano. Por ejemplo, Westlaw, uno de los servicios comerciales de búsqueda legal más grandes, devuelve los documentos en orden cronológico inverso, lo cual es muy efectivo en la práctica [Fuente 10, Archivo: Libro_Stanford.pdf, Página: 52; Fuente 14, Archivo: Libro_Stanford.pdf, Página: 52]. Además, en 2007, la mayoría de los bibliotecarios jurídicos todavía recomendaban el uso de \"términos y conectores\" para búsquedas de alta recuperación, lo que se alinea con este tipo de sistemas [Fuente 10, Archivo: Libro_Stanford.pdf, Página: 52].\n",
      "\n",
      "Existe también un \"modelo booleano extendido\" que permite medir la cercanía de los términos dentro de un documento, limitando el número de palabras intervinientes o haciendo referencia a unidades estructurales como oraciones o párrafos [Fuente 14, Archivo: Libro_Stanford.pdf, Página: 52].\n",
      "Consulta actual del usuario: Cual fue la primera pregunta que te hice?'\n",
      "Contexto recuperado exitosamente\n",
      "\n",
      "----------------\n",
      "\n",
      "Asistente:\n",
      "La primera pregunta que me hiciste fue: \"¿Quien es Marcia Bates?\".\n",
      "\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ciclo de conversación ---\n",
    "print(\"\\n\\n--- INICIANDO ASISTENTE RAG CONVERSACIONAL CON GEMINI ---\")\n",
    "print(\"Escribe 'salir' para terminar la conversación.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nUsuario: \")\n",
    "    if user_query.lower() == 'salir':\n",
    "        break\n",
    "    \n",
    "    final_answer, sources = generate_answer_with_chat(user_query)\n",
    "    print(\"\\n----------------\")\n",
    "    print(\"\\nAsistente:\")\n",
    "    print(final_answer)\n",
    "    print(\"\\n----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f57d87fcefdf3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
